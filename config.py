GPT_124M_CONFIG = {
    "vocab_size": 50257,
    "context_size": 1024,
    "embedding_dim": 768,
    "num_heads": 12,
    "num_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}

GPT_355_CONFIG = {
    "vocab_size": 50257,
    "context_size": 1024,
    "embedding_dim": 1024,
    "num_heads": 16,
    "num_layers": 24,
    "drop_rate": 0.1,
    "qkv_bias": False
}

GPT_774_CONFIG = {
    "vocab_size": 50257,
    "context_size": 1024,
    "embedding_dim": 1280,
    "num_heads": 20,
    "num_layers": 36,
    "drop_rate": 0.1,
    "qkv_bias": True
}

TEST_CONFIG = {
    "vocab_size": 50257,
    "context_size": 1024,
    "embedding_dim": 768,
    "num_heads": 12,
    "num_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": True
}

GPT_1558_CONFIG = {
    "vocab_size": 50257,
    "context_size": 1024,
    "embedding_dim": 1600,
    "num_heads": 25,
    "num_layers": 48,
    "drop_rate": 0.1,
    "qkv_bias": True
}

